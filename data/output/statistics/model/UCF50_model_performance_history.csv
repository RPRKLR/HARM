number_of_classes,image_height,image_width,sequence_length,random_state,model_type,output_layer_activation_function,loss_function,adam_optimizer_learning_rate,training_shuffle,max_training_epochs,batch_size,early_stopping,early_stopping_monitor,early_stopping_mode,early_stopping_patience,accuracy,macro_precision,macro_recall,macro_f1,weighted_precision,weighted_recall,weighted_f1,model_output_path,architecture_and_evaluation_output_path
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,False,val_loss,min,5,0.7741347905282332,0.764166222222227,0.7763123050449942,0.766357080033325,0.7674067672348434,0.7741347905282332,0.7671555573162658,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128_2024-04-03_19:50:17.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-03_19:50:17.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,False,val_loss,min,5,0.7595628415300546,0.7583966551687171,0.7550217798821229,0.7506536716893382,0.7622550459213826,0.7595628415300546,0.7547532461079977,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128_2024-04-02_18:42:29.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-02_18:42:29.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,30,0.697632058287796,0.7029480815089826,0.6981883122509754,0.6966682184589222,0.7092400029610293,0.697632058287796,0.6996109214419062,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_30_2024-04-03_19:06:39.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-03_19:06:39.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,20,0.6994535519125683,0.7236532843802093,0.7064432157873723,0.6978475328648243,0.7265903649778913,0.6994535519125683,0.6963827027659354,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_20_2024-04-02_20:08:27.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-02_20:08:27.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,15,0.6794171220400729,0.699795343310386,0.6703459758724999,0.6739547562485358,0.6999319156755778,0.6794171220400729,0.6798354515289037,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-02_19:59:09.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-02_19:59:09.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,5,0.6775956284153005,0.6794320746000512,0.6799186411876569,0.669406673781134,0.6813224130143007,0.6775956284153005,0.6689532712087852,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_5_2024-04-02_19:32:46.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-02_19:32:46.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,25,0.6648451730418944,0.6610809280585148,0.663137652236757,0.6519271669372821,0.6689609118105461,0.6648451730418944,0.6571420776824346,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_25_2024-04-03_18:54:05.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-03_18:54:05.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,15,0.6502732240437158,0.6450989754911749,0.6505893686032397,0.6380275806918972,0.654078598023293,0.6502732240437158,0.6427650651653897,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-04_06:59:53.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-04_06:59:53.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,10,0.6338797814207651,0.6764344081680996,0.6381883230404534,0.6287266393198765,0.6843839747645198,0.6338797814207651,0.6320347111034937,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_10_2024-04-02_19:40:24.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-02_19:40:24.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,35,0.639344262295082,0.6346993580771071,0.6339384568482994,0.6241198689569041,0.6352299781023852,0.639344262295082,0.627076576790582,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_35_2024-04-03_19:33:48.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-03_19:33:48.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,10,0.6193078324225865,0.636080691449983,0.6184039126182963,0.6130418673846607,0.6417020531848526,0.6193078324225865,0.6153338250186903,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_10_2024-04-04_06:39:22.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-04_06:39:22.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,5,0.5901639344262295,0.6113484926709484,0.589157613385971,0.5793529124990008,0.6127975630236004,0.5901639344262295,0.5810998416106786,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_5_2024-04-04_06:32:14.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-04_06:32:14.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,35,0.8280961182994455,0.8338132781658997,0.8277515708763559,0.8264707263286029,0.834074128827853,0.8280961182994455,0.8267029437256024,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:50:10.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,25,0.8188539741219963,0.8205665545196018,0.8148411888824789,0.8134561672805771,0.826962979102545,0.8188539741219963,0.8197411026702195,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_21:06:07.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,False,val_loss,min,5,0.8170055452865065,0.8194616335096729,0.820620808772393,0.8155093682347602,0.823661191048937,0.8170055452865065,0.8164309639068009,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32_2024-04-05_20:19:41.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,False,val_loss,min,5,0.8170055452865065,0.8121912238690563,0.8160703139773331,0.8078836239699971,0.8290762953624967,0.8170055452865065,0.8162247494858264,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32_2024-04-05_16:47:48.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,4,True,val_loss,min,30,0.8114602587800369,0.8122658306170976,0.805502931076185,0.8060197404301354,0.8178284928307187,0.8114602587800369,0.811841950689877,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_4__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_14:59:34.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,4,False,val_loss,min,5,0.8096118299445472,0.8054169587512201,0.8119354376441971,0.8002637036927821,0.8221878004588968,0.8096118299445472,0.8097402119619451,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_4_2024-04-05_13:56:01.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,16,False,val_loss,min,5,0.8096118299445472,0.811316019824166,0.8095692400831963,0.8061992854420762,0.8169299243026071,0.8096118299445472,0.8095443057111082,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_16_2024-04-05_19:51:35.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,False,val_loss,min,5,0.8077634011090573,0.8041425919670185,0.8084748786688222,0.7987694192978245,0.820243499486432,0.8077634011090573,0.808289006890219,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256_2024-04-05_17:34:49.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,False,val_loss,min,5,0.8059149722735675,0.8024985409913729,0.8026863480024693,0.7981579897307853,0.8177695722521607,0.8059149722735675,0.8080934066362643,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128_2024-04-05_17:26:24.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,False,val_loss,min,5,0.8040665434380776,0.8051213169566809,0.7916649418393289,0.7919185409498385,0.8153754294941085,0.8040665434380776,0.8037950015138887,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256_2024-04-05_21:01:05.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,16,True,val_loss,min,25,0.800369685767098,0.7970352156421565,0.7911987916542992,0.7864092362541031,0.8116399417467487,0.800369685767098,0.7986824820827783,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_16__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:10:36.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,30,0.7966728280961183,0.7907913738374093,0.7941680262607869,0.7877501012152718,0.8065727325898481,0.7966728280961183,0.7980248748521724,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:32:23.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,15,0.7985212569316081,0.7934421874202073,0.7913861287608531,0.7892414366551731,0.8027983611847762,0.7985212569316081,0.7977058245064012,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:56:08.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,30,0.7985212569316081,0.7865751003678357,0.7929079535495225,0.7841715944885153,0.8054398204744875,0.7985212569316081,0.7977004145658753,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:40:17.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,10,0.7948243992606284,0.7885070252872111,0.789619601383712,0.7840090192545397,0.8052973316814642,0.7948243992606284,0.7952470615790337,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:43:00.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,15,0.7929759704251387,0.7996600991535927,0.7842703353801446,0.7869107538734377,0.8051653604539821,0.7929759704251387,0.7942148544838631,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:17:28.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,35,0.7929759704251387,0.7890093392336602,0.7882022063614135,0.7835185862348618,0.8024458731801434,0.7929759704251387,0.7927756682109559,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:41:30.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,10,0.7929759704251387,0.7894409017581842,0.7889246275559112,0.7827150222068658,0.8010569902656853,0.7929759704251387,0.791473559479842,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:16:09.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,8,False,val_loss,min,5,0.789279112754159,0.7876119178169404,0.7867996288003674,0.7810973232910111,0.8026209276834126,0.789279112754159,0.7898077783370564,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_8_2024-04-05_18:59:56.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,25,0.7874306839186691,0.7873119191677268,0.7881743120311941,0.7783631202047643,0.8073710075698096,0.7874306839186691,0.7890841083763747,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:39:31.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,16,True,val_loss,min,35,0.7874306839186691,0.7923898540191484,0.7839137740162798,0.782460394001983,0.8004011329579197,0.7874306839186691,0.7882782701395223,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_16__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:16:28.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,15,0.789279112754159,0.7875808175696661,0.7942655364339044,0.7817400144524513,0.8028308217976089,0.789279112754159,0.7876154069626444,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:28:17.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,15,0.7874306839186691,0.7844954619418607,0.7824344479949746,0.7793733657943057,0.7935507838093699,0.7874306839186691,0.7865048792086086,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:37:48.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,8,True,val_loss,min,35,0.7874306839186691,0.7845312332471567,0.7866555694081766,0.7828477624648253,0.7898422165565122,0.7874306839186691,0.7860675294653777,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_8__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_19:43:47.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,4,True,val_loss,min,15,0.7855822550831792,0.7780587958585963,0.7825877061652814,0.7752922643241222,0.7942982305126874,0.7855822550831792,0.7856585684756618,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_4__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_14:38:34.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,25,0.789279112754159,0.7937755068234287,0.78879414607132,0.7825863962506637,0.7979171067032684,0.789279112754159,0.7849331708202154,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:57:53.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,10,0.7855822550831792,0.7827667246607887,0.7739076911876815,0.7731024214113921,0.7910566809118222,0.7855822550831792,0.78344608235891,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:55:21.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,5,0.7818853974121996,0.7875850361237221,0.7714754859579525,0.7736458896475427,0.7953097284935096,0.7818853974121996,0.7827256812202181,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:41:51.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,8,True,val_loss,min,25,0.7837338262476895,0.7785378175674228,0.774023172360388,0.7679725452260431,0.7949978994541937,0.7837338262476895,0.7815330525532147,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_8__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_19:32:39.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,30,0.7837338262476895,0.784128836438535,0.7826315990099502,0.7758211785271982,0.792178146607888,0.7837338262476895,0.7811945578065664,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_21:07:14.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,16,False,val_loss,min,5,0.7781885397412199,0.7809100835006408,0.7755188121069923,0.7656738432448952,0.8012658771083715,0.7781885397412199,0.7795584580109245,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_16_2024-04-05_16:14:08.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,16,True,val_loss,min,10,0.7800369685767098,0.7842960374110259,0.7807074234981732,0.7756074532814707,0.7921631609915869,0.7800369685767098,0.7788917492046066,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_16__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:04:56.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,35,0.7744916820702403,0.7725824388031514,0.7627415751782098,0.7627078627012918,0.7874704929621843,0.7744916820702403,0.7764833363754029,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:33:46.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,30,0.7744916820702403,0.7710308614537738,0.7635697893894771,0.7618978745708049,0.7873194979512536,0.7744916820702403,0.7751765872363271,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:58:59.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,8,False,val_loss,min,5,0.7744916820702403,0.7757997628589032,0.7784251864232131,0.768666426325748,0.7904727387909907,0.7744916820702403,0.7748981824155771,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_8_2024-04-05_15:25:42.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,30,0.7744916820702403,0.7701726820683,0.7675012019299599,0.7627884651850699,0.7829301382746593,0.7744916820702403,0.7736532361055999,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:48:35.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,16,True,val_loss,min,15,0.7744916820702403,0.7683746381675295,0.7747505678134371,0.7642775056291569,0.7836173929512402,0.7744916820702403,0.7722763719468418,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_16__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_16:29:59.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,20,0.7744916820702403,0.7650692519416584,0.7683527525697691,0.7598777715053758,0.7844968560524529,0.7744916820702403,0.772127137057688,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:56:58.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,15,0.7781885397412199,0.7734572499891952,0.7787750253832358,0.7701221435211009,0.778565124886929,0.7781885397412199,0.7720949168827853,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:30:06.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,10,0.7726432532347505,0.7672492499757613,0.7726782170705797,0.7625702266293477,0.7833317001583435,0.7726432532347505,0.7716301444052088,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:37:01.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,10,0.7707948243992606,0.7649805137684834,0.766244617847978,0.7594157897589439,0.7806047795637984,0.7707948243992606,0.7711752528757239,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_16:57:08.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,False,val_loss,min,5,0.7707948243992606,0.7697628012919514,0.7687945065579832,0.7574109014376664,0.7892347496889736,0.7707948243992606,0.7693339720414593,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64_2024-04-05_17:09:52.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,8,True,val_loss,min,30,0.767097966728281,0.763305327640041,0.7622329189426267,0.7583514669427858,0.7785222258114025,0.767097966728281,0.7686165112523987,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_8__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_16:02:05.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,False,val_loss,min,5,0.767097966728281,0.7643499331311848,0.7697628438840021,0.7592120098213272,0.783009849205777,0.767097966728281,0.7682317236505365,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64_2024-04-05_20:37:55.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,30,0.7689463955637708,0.7647280757958507,0.7648164215935901,0.7584202068402093,0.7778752583866255,0.7689463955637708,0.7677909171845552,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:22:40.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,4,True,val_loss,min,30,0.7652495378927912,0.7599966723976964,0.7669378535870897,0.7558418176885521,0.780797658711762,0.7652495378927912,0.7672559696767237,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_4__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_18:44:23.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,5,0.7689463955637708,0.7649023956259586,0.7638792666567051,0.7574288823112971,0.7791721936261671,0.7689463955637708,0.7670875827518325,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:28:39.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,35,0.767097966728281,0.7685404863675561,0.7688377826919971,0.7593479178177651,0.7806743234002856,0.767097966728281,0.7670254160639952,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:24:25.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,10,0.7689463955637708,0.7637997711480684,0.7641741952101607,0.7560896298390073,0.7772987292067312,0.7689463955637708,0.7668054654629338,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:29:10.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,4,True,val_loss,min,35,0.767097966728281,0.7681526787217544,0.7533459596375638,0.7547392247047582,0.7779436798598797,0.767097966728281,0.7667242812401921,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_4__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_15:15:37.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,False,val_loss,min,5,0.7634011090573013,0.7674420646048595,0.7543858875602747,0.7517690923880591,0.7863257477047492,0.7634011090573013,0.7661946570866222,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128_2024-04-05_20:52:15.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,16,True,val_loss,min,15,0.7615526802218114,0.7590040944319286,0.751950916311839,0.7474764272167411,0.7770936303381815,0.7615526802218114,0.7628850504246898,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_16__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:06:31.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,20,0.7634011090573013,0.7577201860561434,0.7576335375184976,0.7535583689645691,0.7689915131662556,0.7634011090573013,0.7621906619857838,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:29:38.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,35,0.7615526802218114,0.7648478190477871,0.7594437996668352,0.7571509740266331,0.7693427052647452,0.7615526802218114,0.7609895454185996,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_21:00:01.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,4,False,val_loss,min,5,0.7652495378927912,0.7597972476839588,0.7724433532190831,0.7541625529477302,0.7784423555487546,0.7652495378927912,0.760916858735003,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_4_2024-04-05_17:42:29.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,20,0.7578558225508318,0.7623765635861224,0.7604486271070846,0.7561900300557903,0.7684498028661887,0.7578558225508318,0.7584439991965278,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:00:14.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,4,True,val_loss,min,25,0.7597042513863216,0.7587611230734017,0.7561222879719716,0.7489718611480387,0.7708658137315085,0.7597042513863216,0.758039556975304,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_4__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_18:35:52.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,35,0.7578558225508318,0.765075584789981,0.753779541397398,0.7492304201939025,0.7760003945484596,0.7578558225508318,0.7575134156430258,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:35:57.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,16,True,val_loss,min,25,0.756007393715342,0.7594608664039779,0.7561047295830401,0.7482250936576631,0.7716138443084157,0.756007393715342,0.7557743317309983,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_16__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_16:36:14.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,16,True,val_loss,min,20,0.7578558225508318,0.7616127275617159,0.7598816727118605,0.7512103518693519,0.7698058321697174,0.7578558225508318,0.7549550609126175,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_16__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_16:33:19.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,20,0.756007393715342,0.7498690057256899,0.7538738247414167,0.7428375669100697,0.7703098352887513,0.756007393715342,0.7545399755202347,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:30:54.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,8,True,val_loss,min,5,0.7504621072088724,0.7512585463748962,0.7425434770971746,0.7406128874571858,0.7668538478052044,0.7504621072088724,0.7527975688895457,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_8__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_19:20:33.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,30,0.7541589648798521,0.748692581179456,0.7497487246911745,0.7452791811514493,0.7585736589164463,0.7541589648798521,0.7527595448548577,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:34:06.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,25,0.756007393715342,0.7502061246503138,0.7570936771029482,0.747077867234351,0.7604232594745775,0.756007393715342,0.7515359587320694,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:47:00.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,5,0.7523105360443623,0.7492076453554516,0.751670842232113,0.7400592170124993,0.7689372637570747,0.7523105360443623,0.7511504461717846,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_16:55:43.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,35,0.7541589648798521,0.7483773079293019,0.750342046314652,0.7422141456165073,0.7614680610770096,0.7541589648798521,0.7510461638078848,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_21:08:14.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,8,True,val_loss,min,15,0.7523105360443623,0.763442922647598,0.7535910118796892,0.7480436372295662,0.7674067296458111,0.7523105360443623,0.7501091851790509,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_8__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_19:26:19.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,15,0.7504621072088724,0.7490343895039328,0.7420875015733743,0.7397741351284224,0.759261501237515,0.7504621072088724,0.7491486151085641,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_21:04:20.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,8,True,val_loss,min,30,0.7504621072088724,0.751366298996738,0.7463584455531826,0.737095337390931,0.7661349011980818,0.7504621072088724,0.7488693074949054,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_8__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_19:38:32.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,5,0.7486136783733827,0.7548024128621574,0.7417196710343483,0.7394392249892314,0.7632904500180311,0.7486136783733827,0.7479611949377992,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:54:46.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,4,True,val_loss,min,5,0.7467652495378928,0.7483942483661672,0.7359591115608236,0.7351009130297219,0.755780643219488,0.7467652495378928,0.7445390093975722,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_4__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_18:13:46.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,16,True,val_loss,min,30,0.7430683918669131,0.74277390879581,0.7412305512985853,0.7325468558705086,0.7572878269969369,0.7430683918669131,0.7442772769604961,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_16__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:14:04.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,8,True,val_loss,min,35,0.7430683918669131,0.7393145507796632,0.7454488479907224,0.7342925294444708,0.7581023628740041,0.7430683918669131,0.7436639143802543,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_8__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_16:07:15.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,20,0.7412199630314233,0.7370387306577277,0.7361581739944801,0.7318121381036755,0.7534382776840353,0.7412199630314233,0.7431418301499927,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:45:37.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,25,0.744916820702403,0.7448834609359489,0.7404515670853531,0.7391930612953352,0.7482215682942932,0.744916820702403,0.7429862017888991,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:02:05.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,5,0.7467652495378928,0.7415048719145804,0.748876558264765,0.7416998676161457,0.7463440820902518,0.7467652495378928,0.7429639368502811,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:25:52.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,10,0.7430683918669131,0.7410734403006648,0.7325964558543876,0.7301846310966222,0.7552651711345172,0.7430683918669131,0.7428114972645636,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_21:03:36.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,25,0.7430683918669131,0.7417969220760966,0.7358349759127606,0.7339980235077026,0.7504490236304105,0.7430683918669131,0.741738991216147,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:31:38.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,10,0.7430683918669131,0.7361645021677916,0.735906048309071,0.7292798884948838,0.7526381027220117,0.7430683918669131,0.7416437821119034,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:26:53.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,5,0.7393715341959335,0.731067591041123,0.732752007001362,0.7275825865977678,0.7451693773545384,0.7393715341959335,0.7379684239480366,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:36:30.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,8,True,val_loss,min,5,0.7393715341959335,0.7358540310721309,0.7316068173657633,0.7290805327662728,0.7441182237150095,0.7393715341959335,0.737775317432244,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_8__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_15:45:40.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,15,0.7375231053604436,0.7329953682015636,0.730808249724964,0.7272893788624568,0.743902694799531,0.7375231053604436,0.7361959769749531,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:44:19.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,20,0.7356746765249538,0.7292728114121307,0.7307398205518354,0.7227256182675439,0.7494586646981779,0.7356746765249538,0.7355658654385757,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:38:38.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,5,0.7393715341959335,0.7321975960129656,0.728663954565219,0.7261798934607631,0.7395425127605876,0.7393715341959335,0.7352554950327458,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_21:02:59.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,25,0.7356746765249538,0.7363601551697504,0.733599828317903,0.7283675007660921,0.7440720260862316,0.7356746765249538,0.7350589634637289,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:31:51.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,5,0.7356746765249538,0.7343628324709557,0.735235360551052,0.7302996892089244,0.7419513077511037,0.7356746765249538,0.734343672265171,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:15:10.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,30,0.7356746765249538,0.728481409251075,0.7346920252631516,0.7236809410408606,0.7464746968908248,0.7356746765249538,0.7340108663059459,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:04:22.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,4,True,val_loss,min,20,0.7301293900184843,0.7362280764444988,0.7221034019703435,0.7217950035814965,0.7495334080672166,0.7301293900184843,0.7339411155399174,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_4__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_14:46:42.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,16,True,val_loss,min,35,0.7319778188539742,0.7293614813716871,0.7196387603966425,0.7189848031675529,0.7434112148397242,0.7319778188539742,0.732109620273566,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_16__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_16:43:40.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,4,True,val_loss,min,5,0.7356746765249538,0.7362372002367464,0.7284051149962717,0.7216498802994645,0.7464360364510523,0.7356746765249538,0.7312017584593613,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_4__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_14:30:05.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,16,True,val_loss,min,5,0.7338262476894639,0.7305314758636683,0.7328490308252027,0.7202340610891755,0.747504129379109,0.7338262476894639,0.7298358921746229,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_16__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:02:26.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,20,0.7301293900184843,0.7213802197830212,0.7256482500012409,0.7169729995730509,0.740396167815252,0.7301293900184843,0.729373851880046,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_21:05:15.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,20,0.7282809611829945,0.7186724721650701,0.7216766804736698,0.7148150665076323,0.7312649591483895,0.7282809611829945,0.7255670179480334,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:19:30.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,4,True,val_loss,min,35,0.7245841035120147,0.7279492157107162,0.7227564610606931,0.7158476455425403,0.7429689381703417,0.7245841035120147,0.7236275221513047,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_4__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_18:51:49.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,35,0.7245841035120147,0.72376105607911,0.7126027162676227,0.7134555661628659,0.7297432819261326,0.7245841035120147,0.7230497584696686,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:07:20.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,16,True,val_loss,min,5,0.7264325323475046,0.7251337003389937,0.7130942731809597,0.7100628845791337,0.7332943158413766,0.7264325323475046,0.7212547130130587,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_16__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_16:27:13.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,4,True,val_loss,min,10,0.7208872458410351,0.7098192855006888,0.7120211134644672,0.7027925899862423,0.7324737001007318,0.7208872458410351,0.7195416653057928,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_4__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_18:17:33.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,8,True,val_loss,min,10,0.722735674676525,0.7269280238626082,0.7095169222533794,0.7071892857957617,0.7349405355135858,0.722735674676525,0.7188604979573426,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_8__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_19:23:02.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,25,0.722735674676525,0.7209590239354513,0.7291650672467433,0.7141617733155934,0.7307040758177383,0.722735674676525,0.7167759862303541,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_17:20:49.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,4,True,val_loss,min,15,0.711645101663586,0.6968605738657809,0.7026609154948293,0.693966715330889,0.7186906074439875,0.711645101663586,0.7107840718231242,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_4__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_18:22:42.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,8,True,val_loss,min,20,0.711645101663586,0.7061214088511523,0.6921281652633899,0.6899709356709176,0.7270634387545963,0.711645101663586,0.710415451014794,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_8__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_15:54:16.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,4,True,val_loss,min,10,0.7079482439926063,0.7089606203685488,0.7063768812617257,0.7002681047297119,0.7243607156927687,0.7079482439926063,0.7090660658500368,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_4__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_14:33:54.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,16,True,val_loss,min,30,0.7042513863216266,0.7051919706703507,0.6977368673929767,0.6912611871680745,0.7192692814304142,0.7042513863216266,0.7030315638549018,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_16__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_16:40:35.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,8,True,val_loss,min,25,0.7005545286506469,0.6961563608611363,0.7018519745093077,0.6919479370541366,0.7108474872844718,0.7005545286506469,0.6992539365176096,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_8__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_15:57:58.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,4,True,val_loss,min,25,0.7042513863216266,0.6938409645279949,0.7071888731675033,0.6903298324465844,0.7123981080091287,0.7042513863216266,0.6992482012609075,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_4__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_14:52:36.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,16,True,val_loss,min,20,0.6987060998151571,0.694874351417234,0.6985703278168609,0.6864113668018703,0.7096624408086686,0.6987060998151571,0.6932972395599696,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_16__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_20:08:39.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,8,True,val_loss,min,10,0.6894639556377079,0.6837781628329895,0.6703903545212028,0.6692079032592455,0.6978775469299385,0.6894639556377079,0.6859447888784875,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_8__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_15:48:47.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,15,0.6802218114602587,0.6732713264615438,0.670037942829652,0.6636383895640883,0.6969155849578578,0.6802218114602587,0.6803594296454238,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_16:58:32.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,8,True,val_loss,min,20,0.6820702402957486,0.6783207307225079,0.6811207136724433,0.6630866372692017,0.6999186420202619,0.6820702402957486,0.6753881865497298,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_8__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_19:29:26.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,8,True,val_loss,min,15,0.6691312384473198,0.6666984867804479,0.6703002051469563,0.6597328150250668,0.6857510159934507,0.6691312384473198,0.6690981657584983,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_8__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_15:51:10.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,4,True,val_loss,min,20,0.6451016635859519,0.6577906610369408,0.6490732701904323,0.6371535133126287,0.6745770998195895,0.6451016635859519,0.6455941584550204,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_4__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_18:29:41.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,5,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,16,True,val_loss,min,10,0.6395563770794824,0.6487833938585392,0.6399460425458805,0.627034472846643,0.6654931770637981,0.6395563770794824,0.6380942767645695,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_16__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-05_16:28:32.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_08:29:48.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,False,val_loss,min,5,0.8499095840867993,0.8413423131331894,0.8406437077217325,0.8383491132975633,0.8509583674207455,0.8499095840867993,0.8480279361287856,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128_2024-04-06_06:39:10.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,False,val_loss,min,5,0.8481012658227848,0.8420547508444658,0.8405435587870583,0.8397883278055047,0.8493768557476109,0.8481012658227848,0.8472273228655959,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128_2024-04-06_08:14:34.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,False,val_loss,min,5,0.840867992766727,0.8296997060657967,0.8311475538184958,0.828026463068945,0.84184769072168,0.840867992766727,0.8391652898207763,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64_2024-04-06_07:49:19.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,False,val_loss,min,5,0.840867992766727,0.8310983982137273,0.8267401284957246,0.8258290641535876,0.8427688374524198,0.840867992766727,0.8390952404731111,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64_2024-04-06_06:12:03.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,False,val_loss,min,5,0.8354430379746836,0.8292339801264287,0.82347044822944,0.8242629106342554,0.8393895503252206,0.8354430379746836,0.8351405462093128,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32_2024-04-06_07:09:12.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,False,val_loss,min,5,0.8352272727272727,0.8281084172006518,0.8322572461757145,0.8255488890182325,0.8396097062414916,0.8352272727272727,0.8328840379539084,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64_2024-04-06_01:27:34.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,25,0.8352272727272727,0.830457015461557,0.8312216247701285,0.8276432921977321,0.836626543610671,0.8352272727272727,0.8328649411355488,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_01:16:37.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,False,val_loss,min,5,0.8318264014466547,0.8292402090843061,0.8192770844263565,0.8218088783351435,0.8359073508676755,0.8318264014466547,0.8314593929960269,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256_2024-04-06_08:30:42.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,30,0.8314393939393939,0.8320406890718338,0.8258143706697462,0.8260593061922288,0.8317180720685796,0.8314393939393939,0.8286884397249584,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_01:21:11.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,30,0.8314393939393939,0.827778442382537,0.824913446295431,0.8212774706726481,0.8334447049434752,0.8314393939393939,0.8275242455066526,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_00:50:33.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,False,val_loss,min,5,0.8257575757575758,0.8332426750736373,0.819971762577625,0.818750542521219,0.8344847686055357,0.8257575757575758,0.8228889483011438,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32_2024-04-06_01:02:54.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,35,0.8227848101265823,0.8161283468236793,0.8133869363583646,0.8120948903300349,0.8283521877909964,0.8227848101265823,0.8227887253411762,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_06:34:55.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,25,0.8209764918625678,0.8128736187880687,0.8085076038090635,0.8083342721136246,0.8232291062622961,0.8209764918625678,0.819479547278435,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_07:33:16.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,False,val_loss,min,5,0.821969696969697,0.8160644908363024,0.8225346400294511,0.8148725784825969,0.824548556640207,0.821969696969697,0.8192123047260953,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256_2024-04-06_00:53:55.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,False,val_loss,min,5,0.8200757575757576,0.8178017779310378,0.8220393587765067,0.8155170819962867,0.8240400129162612,0.8200757575757576,0.818196805409979,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128_2024-04-06_01:43:25.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,30,0.8125,0.8205184479610523,0.8135562317696794,0.8111920028299779,0.8215284830074058,0.8125,0.8114675224302506,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_01:38:28.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,35,0.8125,0.8160708350230269,0.8166971865927204,0.811297570876422,0.8187139952997211,0.8125,0.8105785922771319,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_01:58:24.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,35,0.8010849909584087,0.8001757285052686,0.7921409156608602,0.7917127392603523,0.8178336280627925,0.8010849909584087,0.805318850918931,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_08:11:04.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,30,0.8068181818181818,0.8004925096592925,0.8015091155553866,0.7968919152511745,0.8063250216367717,0.8068181818181818,0.8026561244961785,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_00:59:47.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,False,val_loss,min,5,0.8068181818181818,0.8005253405323234,0.7972273299456317,0.7933652276204921,0.8083082808561278,0.8068181818181818,0.8023883605100339,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64_2024-04-06_00:29:04.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,20,0.8010849909584087,0.795924843977912,0.7883906095618236,0.7890055911091134,0.8082261865148829,0.8010849909584087,0.8019568772624788,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_06:25:02.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,35,0.8011363636363636,0.7953448907595075,0.8054312446393677,0.7943285134703405,0.8072897161044004,0.8011363636363636,0.7987926844580163,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_01:01:13.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,False,val_loss,min,5,0.7992424242424242,0.7957189273004435,0.8018855249754374,0.7922526638177281,0.807715471694317,0.7992424242424242,0.797850276161069,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128_2024-04-06_00:45:10.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,35,0.7992766726943942,0.7931049154916316,0.793143804324993,0.7858442641516609,0.8104579730117567,0.7992766726943942,0.7977534502987816,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_07:44:08.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,False,val_loss,min,5,0.8049242424242424,0.8006542936114682,0.8061734524869323,0.7919773688981713,0.8100198008086509,0.8049242424242424,0.7965152520543365,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32_2024-04-06_00:02:07.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,False,val_loss,min,5,0.7988826815642458,0.805325305574011,0.8085852358051749,0.801717851469394,0.8031282563706935,0.7988826815642458,0.7961731669925237,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128_2024-04-07_22:24:25.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,False,val_loss,min,5,0.7951582867783985,0.8011378621096792,0.7967856096445806,0.7943631181819025,0.8047993994691055,0.7951582867783985,0.7956576711095718,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64_2024-04-07_21:59:45.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,False,val_loss,min,5,0.7970204841713222,0.8000294663007166,0.8019719705881133,0.7954553701622726,0.8059111548500302,0.7970204841713222,0.7956068155941247,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128_2024-04-07_13:10:18.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,30,0.7974683544303798,0.7934816775576469,0.791046662503152,0.7860226488975891,0.801389742547289,0.7974683544303798,0.7934059260621792,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_08:07:52.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,False,val_loss,min,5,0.7920433996383364,0.783597308652329,0.7772278867969243,0.776823459160968,0.8004677940240738,0.7920433996383364,0.7930355266183512,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256_2024-04-06_06:54:02.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,30,0.7916666666666666,0.7932429765909095,0.7950085658593036,0.7876093285227675,0.8037822875192479,0.7916666666666666,0.7922302710176439,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_00:20:57.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,35,0.7902350813743219,0.7817622631063743,0.7748054654905295,0.7755065782411374,0.7966343340300307,0.7902350813743219,0.7908843644497698,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_08:28:18.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,35,0.7916666666666666,0.7937309485871127,0.7989973605483754,0.7902238474571112,0.8014752453803559,0.7916666666666666,0.7904476261534029,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_01:40:43.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,20,0.7884267631103075,0.7782136202893566,0.7797679770937365,0.7752334075589408,0.7988830943080999,0.7884267631103075,0.7904047091044848,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_08:22:38.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,35,0.7878787878787878,0.7973948195065949,0.7888691625926889,0.7874890600200347,0.801788835702175,0.7878787878787878,0.7901587273152849,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_00:23:55.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,25,0.7935606060606061,0.7840862315963222,0.7919088724512611,0.7836977130218246,0.792171726608301,0.7935606060606061,0.7887097575759948,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_00:58:20.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,25,0.786618444846293,0.7933648098030609,0.7715261143660328,0.7738574471517109,0.8040989523924567,0.786618444846293,0.7879581921668974,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_06:47:54.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,20,0.7916666666666666,0.780481946255354,0.7886582215968077,0.7787155464476533,0.7932196073467744,0.7916666666666666,0.7867691832561897,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_00:57:07.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,30,0.786618444846293,0.7838837959032108,0.774436330532911,0.771602920822071,0.8009271744219956,0.786618444846293,0.7863612068105752,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_07:39:45.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,30,0.7859848484848485,0.7937938834426459,0.780899728393861,0.7786480111398629,0.8023226188890649,0.7859848484848485,0.7854321181265707,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_01:49:09.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,False,val_loss,min,5,0.7858439201451906,0.7832861727753586,0.7797574944856168,0.7741276924761029,0.7953591767698378,0.7858439201451906,0.783775787070931,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128_2024-04-06_20:26:22.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,False,val_loss,min,5,0.7859848484848485,0.7799376725910084,0.7830317402089099,0.7750646789621249,0.7943289552653313,0.7859848484848485,0.7837310825114299,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256_2024-04-06_01:51:37.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,30,0.783001808318264,0.7715336871674399,0.7702016340482745,0.7669844206348132,0.7900131196548975,0.783001808318264,0.7825619048045197,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_06:49:41.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,25,0.7811934900542495,0.7754032150498481,0.7669254604024986,0.7673512865002181,0.7893068982031808,0.7811934900542495,0.7816204190561302,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_06:28:16.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,35,0.783001808318264,0.7800557512693042,0.7673282270453738,0.7685462961586371,0.7889690873225392,0.783001808318264,0.7816197965764539,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_06:06:46.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,30,0.779385171790235,0.7663387675890724,0.7636533171239195,0.7616785437893521,0.7870521103647473,0.779385171790235,0.7799477429992993,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_08:26:15.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,False,val_loss,min,5,0.7803992740471869,0.7762107209952498,0.7758825765913917,0.7720606178975158,0.784877615138228,0.7803992740471869,0.778826985245023,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256_2024-04-06_18:39:45.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,20,0.779385171790235,0.7734289155600157,0.7669915165674283,0.7648062085165125,0.7846064677839695,0.779385171790235,0.7764113606194366,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_07:00:56.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,30,0.783001808318264,0.768374594519187,0.7623674822357557,0.758906790889112,0.7797545391797572,0.783001808318264,0.7755273241014867,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_06:01:27.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,35,0.776536312849162,0.7864202599161997,0.7810937466275527,0.7756289072221645,0.7875459147250451,0.776536312849162,0.7749726422952352,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_13:04:37.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,25,0.7765151515151515,0.7760513616223039,0.7718766649266999,0.7672630992794747,0.7809195749984874,0.7765151515151515,0.7730515132279338,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_00:37:55.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,30,0.7728119180633147,0.7766530809640113,0.7764166404949513,0.7736696545447961,0.7758445837790393,0.7728119180633147,0.771543578768401,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_21:23:00.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,35,0.7746212121212122,0.7724317349070008,0.7768668710051705,0.7677925651966444,0.7814641531845457,0.7746212121212122,0.7715208660058006,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_00:52:31.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,False,val_loss,min,5,0.7777777777777778,0.7698580439049527,0.7771536253137338,0.7687114310561565,0.7748677365117744,0.7777777777777778,0.7715189604437419,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256_2024-04-08_21:35:04.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_18:29:03.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,25,0.7685352622061483,0.7660339269299379,0.7519519358435678,0.7542334213934407,0.7802918663520646,0.7685352622061483,0.7699269287461382,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_08:24:32.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,30,0.7746212121212122,0.7752723348341044,0.771395700137796,0.7643015858169894,0.7807874529302242,0.7746212121212122,0.7693448423170374,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_00:39:57.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,False,val_loss,min,5,0.7685352622061483,0.770576117083613,0.7611253458166521,0.7551328117500563,0.7904900904450094,0.7685352622061483,0.7687503709272764,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32_2024-04-06_05:30:12.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,20,0.7708333333333334,0.7693321933657687,0.7718423311210811,0.766303174010493,0.7743850006199281,0.7708333333333334,0.768635691312319,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_01:54:49.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,False,val_loss,min,5,0.7676950998185118,0.7731335294916745,0.764058360915249,0.7593763641481136,0.7864878630780142,0.7676950998185118,0.7684079289544029,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32_2024-04-06_13:14:43.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,25,0.7670454545454546,0.7681818979321327,0.7715432091833314,0.7626203271558158,0.783261692689337,0.7670454545454546,0.7679257315084728,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_01:48:03.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,25,0.7685352622061483,0.7560878479087837,0.7527763955964941,0.7507780636798742,0.770696980295693,0.7685352622061483,0.7659026358825991,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_08:39:16.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,False,val_loss,min,5,0.7672253258845437,0.7822411906809262,0.771054351077421,0.7706428390583229,0.7761373799987022,0.7672253258845437,0.7655801388747606,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32_2024-04-07_11:58:02.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,False,val_loss,min,5,0.7653631284916201,0.771371267681012,0.778945559310747,0.7706086299211723,0.7711716545951773,0.7653631284916201,0.7635662385603458,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256_2024-04-07_22:52:39.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,20,0.7622504537205081,0.7621447379607343,0.7522213420967381,0.7535611440959147,0.7717498485992844,0.7622504537205081,0.7635358572536648,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_19:30:38.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,False,val_loss,min,5,0.7676950998185118,0.7656570014356517,0.7560114533425869,0.7527275883283908,0.7733909278550101,0.7676950998185118,0.7632617385151909,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64_2024-04-06_14:20:37.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,35,0.7653631284916201,0.7714776320728208,0.7660734138721005,0.7622319615384848,0.7731841984569607,0.7653631284916201,0.7630558916168932,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_21:55:54.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,25,0.7651515151515151,0.7652432781424118,0.7604196914697328,0.7588443782734354,0.7682346995368361,0.7651515151515151,0.7626565993335916,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_00:49:25.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,20,0.7622504537205081,0.757299396963857,0.7508356984703696,0.7492471962854681,0.770928794097593,0.7622504537205081,0.7623395900039728,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_14:38:17.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,25,0.7613636363636364,0.7560339527679926,0.761259128014471,0.7538317326100084,0.7699474390782246,0.7613636363636364,0.7614883045287762,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_01:55:56.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,False,val_loss,min,5,0.7653631284916201,0.7669418649359443,0.7757641528772059,0.7653212808861234,0.7688212767537767,0.7653631284916201,0.7612748211990389,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32_2024-04-07_21:33:38.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,20,0.7649186256781193,0.7620202159749302,0.7485042088561179,0.7459634203060954,0.7740063603169205,0.7649186256781193,0.7607486555334847,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_08:02:31.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,30,0.755877034358047,0.7698722786968177,0.7411563182693461,0.7454401428691072,0.7835234469524434,0.755877034358047,0.7605608396670294,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_06:31:45.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,20,0.759469696969697,0.7591617179163871,0.7620266092863428,0.7544773526316719,0.7700155116735865,0.759469696969697,0.7586228728338092,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_01:46:50.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,False,val_loss,min,5,0.7607843137254902,0.7621052888941421,0.7649133994387277,0.7598796348958694,0.7620384211231782,0.7607843137254902,0.7576404898567359,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64_2024-04-07_03:01:08.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,False,val_loss,min,5,0.7586206896551724,0.7596898862938335,0.754493036907662,0.7512025526199635,0.7664820164539258,0.7586206896551724,0.7572905760247235,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256_2024-04-06_20:48:27.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,False,val_loss,min,5,0.7577413479052824,0.7633970636016515,0.7614076149024008,0.7546506133249684,0.7690154536150844,0.7577413479052824,0.7562691445899202,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32_2024-04-08_08:23:47.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_07:22:15.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,25,0.755877034358047,0.7468637548287691,0.7433838334177707,0.7403047195860049,0.7624692386237181,0.755877034358047,0.7549484535694568,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_08:05:16.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,25,0.7613636363636364,0.7539541685912453,0.7651047116346289,0.7513065937749683,0.7638637402664331,0.7613636363636364,0.75468778835778,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_01:36:37.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,30,0.7540687160940326,0.7400649309365963,0.7365774377629171,0.734162555618189,0.7615986611897333,0.7540687160940326,0.7543702165118693,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_08:41:06.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,False,val_loss,min,5,0.7568058076225045,0.7516434836490804,0.7464218216600486,0.7443348395113386,0.7596352837656309,0.7568058076225045,0.7541524980999769,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32_2024-04-06_19:02:48.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,35,0.7523277467411545,0.7732463018453556,0.7629463741473692,0.7591902916263148,0.7750690246274541,0.7523277467411545,0.7531538510912016,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_22:20:59.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,False,val_loss,min,5,0.7531760435571688,0.7532464630932657,0.7512056848572327,0.7471993069163537,0.7624903666273221,0.7531760435571688,0.7529894628230596,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128_2024-04-06_18:16:22.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,20,0.7504520795660036,0.746350532540197,0.7330379688349812,0.7293735308383302,0.7719303874387996,0.7504520795660036,0.7526919732717331,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_07:29:46.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,35,0.7522768670309654,0.7573789021852123,0.7506060032172619,0.7478615387874663,0.7640000273463555,0.7522768670309654,0.7520250184839935,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_15:03:56.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_14:11:50.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,20,0.7522603978300181,0.7468991008098136,0.7414812494154643,0.7378067699071698,0.7624526570343293,0.7522603978300181,0.7513928667682096,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_08:37:36.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,30,0.7556818181818182,0.7513117441207314,0.7612675610662601,0.7455026632025731,0.7661258703619179,0.7556818181818182,0.7507747354322075,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_01:57:09.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,20,0.750465549348231,0.7637732935471708,0.7593733312673379,0.7497963285723825,0.7736629264942518,0.750465549348231,0.7496974089733556,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_21:47:51.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,False,val_loss,min,5,0.7513611615245009,0.7495465523413348,0.7474689433059096,0.740903907122043,0.7613383097876705,0.7513611615245009,0.7496598639873852,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64_2024-04-06_17:43:13.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,False,val_loss,min,5,0.750465549348231,0.7521679588559984,0.7559139665138617,0.7491782976974855,0.7561470067477274,0.750465549348231,0.748809818289078,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256_2024-04-07_20:58:52.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,20,0.7522768670309654,0.7584087230942053,0.7503906122117571,0.7449232708329292,0.7639742267901379,0.7522768670309654,0.7485942942804922,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_09:59:38.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_07:22:15.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,35,0.7540687160940326,0.7485616751459927,0.7377166150795829,0.7369301680761156,0.7531963079287332,0.7540687160940326,0.747403156825597,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_06:51:53.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,False,val_loss,min,5,0.7523277467411545,0.750351892672439,0.7631116781928914,0.7517999331538361,0.7530644947720472,0.7523277467411545,0.7471636612288346,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64_2024-04-07_12:36:07.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,25,0.7504553734061931,0.7506600507091012,0.7469940457405937,0.7430364850204062,0.7531926681355688,0.7504553734061931,0.7460381800633655,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_10:07:33.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_07:22:15.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,False,val_loss,min,5,0.7450980392156863,0.7495815353326762,0.7410797772112752,0.7414734816723603,0.7531377081486457,0.7450980392156863,0.7455417994091842,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128_2024-04-07_06:12:13.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,False,val_loss,min,5,0.7477313974591652,0.7609359577384407,0.7429763687260416,0.7373773057154986,0.7704388309389633,0.7477313974591652,0.7452529951163555,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64_2024-04-06_19:54:19.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,False,val_loss,min,5,0.7468123861566485,0.7464415248794631,0.7493197972666253,0.7445616433063209,0.7497048677851718,0.7468123861566485,0.7449672936823256,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64_2024-04-08_14:11:50.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_14:11:50.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,25,0.7467411545623837,0.7539213646475477,0.7546036013759038,0.7483280441022884,0.754971893770865,0.7467411545623837,0.744881079421683,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_21:18:22.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,25,0.7467411545623837,0.7536173882480502,0.7496351045685785,0.7452731726859373,0.7557128695500808,0.7467411545623837,0.7448518079264816,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_13:31:18.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,False,val_loss,min,5,0.7470588235294118,0.7478940319911909,0.7554203556523569,0.7456191537517317,0.752163977174574,0.7470588235294118,0.7432633952869175,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32_2024-04-07_02:00:03.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,30,0.7441016333938294,0.742435003945559,0.7375848372657245,0.73594744009443,0.7498039020871192,0.7441016333938294,0.7432551777813506,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_20:17:57.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,35,0.7450271247739603,0.7382054324404081,0.7355564999436383,0.7299649377895551,0.7517996179315086,0.7450271247739603,0.7418633011567408,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_08:43:11.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,35,0.7414104882459313,0.7357526900276594,0.7261679218687505,0.7222204924197713,0.7553469601641593,0.7414104882459313,0.7404774795010056,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_07:06:37.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,35,0.7386363636363636,0.7366368158546317,0.7326379233943694,0.7299781788064588,0.7457469959941994,0.7386363636363636,0.7377812394618442,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_01:24:28.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,20,0.7367424242424242,0.7379139581832315,0.7370682910754799,0.7290344079729486,0.7497480414117044,0.7367424242424242,0.7363854974842182,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_01:34:54.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,30,0.7396021699819169,0.731779204271122,0.7170172058768614,0.7173374430961081,0.7434279725653168,0.7396021699819169,0.7362971706831952,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_07:04:39.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,30,0.7386569872958257,0.7336399455878193,0.7369716856374697,0.7268082885924172,0.7475104523243186,0.7386569872958257,0.7347982071842057,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_14:01:05.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,35,0.7374301675977654,0.7362375641694789,0.7425970683014402,0.7347902365688125,0.7401244638216133,0.7374301675977654,0.7341667349152545,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_20:54:49.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,20,0.7367424242424242,0.7374662614197686,0.7344731136648356,0.7307003852586199,0.7401891425185377,0.7367424242424242,0.7336392418510637,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_01:14:25.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,False,val_loss,min,5,0.7431693989071039,0.7380093037716458,0.7433449133902537,0.7304009103870748,0.7432443250740317,0.7431693989071039,0.7332377397445511,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32_2024-04-08_18:29:03.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_18:29:03.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,30,0.7368421052631579,0.7351909461527628,0.7194198629069307,0.720340327091824,0.7417145376602796,0.7368421052631579,0.7329588252943189,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_19:41:19.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,False,val_loss,min,5,0.7377049180327869,0.736844827969704,0.7401553954220232,0.7312878597683868,0.7418841522588498,0.7377049180327869,0.7328883309319117,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64_2024-04-08_09:27:49.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_07:22:15.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,35,0.7386363636363636,0.7381568568226766,0.7361426491586346,0.7283349568113654,0.7443941625871012,0.7386363636363636,0.732437668777014,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_00:43:04.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,35,0.73502722323049,0.727474532807108,0.7334682333024843,0.7236726009678675,0.7414874294915904,0.73502722323049,0.7313396159206527,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_14:11:08.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,25,0.7337057728119181,0.7455584210322104,0.735649423553105,0.7336434124998322,0.7412313913360853,0.7337057728119181,0.7304270929621077,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_22:15:18.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,20,0.7269439421338155,0.7436422441392406,0.7115385203281186,0.7181259372906511,0.7461411373928546,0.7269439421338155,0.727815783499877,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_06:46:20.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,30,0.7281191806331471,0.7360640423491825,0.7345670796462767,0.728615640509889,0.7392966809959339,0.7281191806331471,0.726884960568757,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_13:36:37.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,False,val_loss,min,5,0.73224043715847,0.7266678587107719,0.7327110594902707,0.7221178005779242,0.7303017582304508,0.73224043715847,0.7239284220101467,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256_2024-04-08_16:20:48.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_14:11:50.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,30,0.7206703910614525,0.7442589688293053,0.7294811564486924,0.7261028387664878,0.7462362085284797,0.7206703910614525,0.7231784032673839,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_21:52:49.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,False,val_loss,min,5,0.7231329690346083,0.728791307534286,0.7273969199228877,0.7192834723123288,0.7387178206415175,0.7231329690346083,0.7230375563977613,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32_2024-04-08_17:37:54.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_14:11:50.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,25,0.7223230490018149,0.7179343845803567,0.7112716434069293,0.7095060063283796,0.7312193079632098,0.7223230490018149,0.722046282231304,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_18:53:21.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,35,0.7253787878787878,0.7246726592953591,0.7207220190843898,0.717742301174088,0.7274528901711014,0.7253787878787878,0.7219469602735304,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_01:50:19.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,30,0.7241379310344828,0.7187373523024709,0.7169306412943764,0.7126674020490908,0.7286306760478161,0.7241379310344828,0.7211920821807131,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_18:31:46.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,20,0.7215909090909091,0.7086077412601616,0.7201754653867138,0.7080658946382398,0.7249397964063766,0.7215909090909091,0.7179848640561958,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_00:48:36.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,25,0.7225325884543762,0.731940594350534,0.7272907862711929,0.7220338169558503,0.727613922127207,0.7225325884543762,0.7178216776539134,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_20:47:59.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,20,0.7188081936685289,0.7307639111475097,0.7239233216751069,0.7235588576388741,0.7226983730007707,0.7188081936685289,0.7172293416945427,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_13:26:40.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,False,val_loss,min,5,0.7196078431372549,0.7236487416847324,0.7167551531254774,0.7142818816946008,0.7245670317531593,0.7196078431372549,0.7164814033764443,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128_2024-04-07_03:38:20.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,35,0.7186932849364791,0.7162167353773417,0.7061568505884273,0.7028406072997879,0.7237508135514013,0.7186932849364791,0.7141831092986481,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_20:22:02.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,35,0.7150635208711433,0.725034876184257,0.7035285112372539,0.6993179906599378,0.739579615602945,0.7150635208711433,0.7139505207960788,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_18:11:19.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,25,0.7169459962756052,0.7191382833331343,0.7208728438591493,0.712689419871909,0.7230760552705631,0.7169459962756052,0.7137760938242995,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_22:41:51.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,False,val_loss,min,5,0.7176684881602914,0.7160142423283904,0.7173262193453498,0.7126315075639515,0.7175707822362167,0.7176684881602914,0.7134369460099431,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64_2024-04-08_19:30:23.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_18:29:03.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,25,0.7113594040968343,0.7329529699544035,0.713951935011957,0.7140905322952042,0.7319683089442691,0.7113594040968343,0.7125901222340212,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_23:11:47.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,30,0.7113594040968343,0.7280148432699871,0.7172554285046047,0.7134929998518175,0.7327547459386984,0.7113594040968343,0.7124198706183967,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_12:59:35.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,20,0.7234848484848485,0.7421362620410005,0.7210339781698265,0.7056803922150878,0.7465573104962636,0.7234848484848485,0.7123222935716187,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_00:16:20.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,20,0.7094972067039106,0.7356163208202584,0.7110792760619986,0.7091793236324586,0.7406730019443716,0.7094972067039106,0.7120640530216791,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_22:39:13.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,35,0.7168784029038112,0.7215352871242964,0.7105779456737851,0.7040539655767872,0.726900872956033,0.7168784029038112,0.7112679027262476,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_19:47:55.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,20,0.7094972067039106,0.7165836304437627,0.7193059493202183,0.7114392211060938,0.7231309100868928,0.7094972067039106,0.7096655161723496,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_12:51:22.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,35,0.7176684881602914,0.7108354725008647,0.7168818269305859,0.7047478167466298,0.7183008905950761,0.7176684881602914,0.7096639458387497,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_21:25:43.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_18:29:03.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,25,0.705989110707804,0.7180585009737421,0.6929556876781222,0.6967980441711119,0.7290309478376159,0.705989110707804,0.7085749940381262,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_20:39:20.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,35,0.707635009310987,0.7253832363611776,0.7160327530635693,0.714818106081591,0.7228373385852299,0.707635009310987,0.7085725139494982,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_23:19:54.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,25,0.7094972067039106,0.7162835818340265,0.7097793663185965,0.7097173094110494,0.7130794186692908,0.7094972067039106,0.7074996718556876,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_12:55:03.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,35,0.707635009310987,0.7171412832389992,0.7134740220497238,0.7122414896647076,0.7109740925007331,0.707635009310987,0.7059937992244224,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_21:28:50.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,20,0.7049180327868853,0.7159812912530303,0.7086536974321107,0.7037751143091147,0.7217819836098127,0.7049180327868853,0.7054045159973166,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_19:01:49.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_18:29:03.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,35,0.7057728119180633,0.711842926804633,0.7143648133875901,0.7049205369291711,0.7179945882535845,0.7057728119180633,0.7047489509941186,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_22:48:40.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,20,0.7114337568058077,0.7138173275724231,0.7074851672313253,0.6958889746477653,0.7236186283206508,0.7114337568058077,0.703493883847629,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_13:47:15.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,25,0.7049180327868853,0.7098287007801638,0.7004807990357331,0.6979885026185815,0.713788009477304,0.7049180327868853,0.7029007793554296,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_21:10:01.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_18:29:03.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,20,0.707635009310987,0.7082021977878845,0.7088626956049932,0.7003707559007918,0.712160267610459,0.707635009310987,0.7022099953235386,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_20:45:14.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,False,val_loss,min,5,0.7140255009107468,0.7023591648741816,0.7135797583304657,0.6983297366065776,0.7079107828571563,0.7140255009107468,0.7014984415072011,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256_2024-04-09_06:30:16.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-09_05:59:38.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,30,0.7039106145251397,0.7133975916504739,0.6973682764451391,0.6968306913606136,0.7147252864617774,0.7039106145251397,0.7007028744918034,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_20:51:07.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,False,val_loss,min,5,0.707635009310987,0.7054609410514642,0.7148879720100134,0.7021622245933884,0.708476989206461,0.707635009310987,0.700679694201249,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128_2024-04-07_20:29:04.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,25,0.7064393939393939,0.7161890202694764,0.6963139624280682,0.6909996738941487,0.7214061580056442,0.7064393939393939,0.6997950846492014,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_00:18:43.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,35,0.7005444646098004,0.6994784049063771,0.6863852643089798,0.6866417201014277,0.709330883186888,0.7005444646098004,0.6990194403749668,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_18:34:54.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,35,0.7012750455373407,0.7272915917006841,0.7008409935849815,0.6921509754459945,0.7354391388318362,0.7012750455373407,0.6971947423044523,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_19:22:19.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_18:29:03.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,30,0.7001862197392924,0.7097532992622579,0.7078343425296201,0.6991783371764317,0.7112387439392007,0.7001862197392924,0.6971601903307734,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_23:15:47.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,20,0.6994535519125683,0.702817586789492,0.6958725753319868,0.6923181499453348,0.7062481023307385,0.6994535519125683,0.6963730452119488,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_20:01:26.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_18:29:03.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,20,0.6943942133815552,0.7177753535118876,0.6835658248180809,0.6868545592357748,0.7263787361684018,0.6943942133815552,0.6957840786926217,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_05:54:31.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,25,0.6964618249534451,0.7052228890145064,0.6973096315982751,0.6935959644571915,0.707837262429083,0.6964618249534451,0.6952165253455818,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_12:22:01.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,25,0.6980108499095841,0.681718550584767,0.6792056738247453,0.674217112803781,0.7014391778828208,0.6980108499095841,0.693627823516996,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_07:03:01.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,20,0.6903460837887068,0.7151624731057242,0.6867826919088283,0.6897797999544082,0.7196234645691283,0.6903460837887068,0.6933250808020891,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_18:10:43.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_14:11:50.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,35,0.6987295825771325,0.70842126293076,0.6849247731226098,0.6831537588921038,0.7068618005246013,0.6987295825771325,0.6917210830138126,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_18:58:51.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,20,0.6950757575757576,0.7030239636304436,0.6956144882968761,0.6864336987694322,0.7090575819929246,0.6950757575757576,0.690613842666883,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_00:36:28.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,25,0.6932849364791288,0.6926131232534073,0.6815515216790139,0.6763272799119303,0.7055602847911018,0.6932849364791288,0.6893281140921195,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_19:35:51.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,35,0.6903460837887068,0.7153795611419134,0.6861464753414384,0.6822739490918265,0.7198907688598287,0.6903460837887068,0.6855151827139672,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_16:11:29.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_14:11:50.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,35,0.6871508379888268,0.6995803359202369,0.6933976632651668,0.6874964351226175,0.7000612832853275,0.6871508379888268,0.6843236073313517,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_13:43:21.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,30,0.6867030965391621,0.6891090090212425,0.6799829991790867,0.6751859569505585,0.6976452438445272,0.6867030965391621,0.6829157618262393,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_14:56:00.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_14:11:50.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,35,0.6867030965391621,0.6999519078797796,0.6869229289183435,0.6816653237025109,0.7015988960845417,0.6867030965391621,0.6827150014138847,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_09:18:59.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_07:22:15.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,False,val_loss,min,5,0.6823529411764706,0.7040726275572307,0.6865644352295536,0.6838181482087647,0.7065070016405952,0.6823529411764706,0.6825101964799,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32_2024-04-07_04:36:07.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,20,0.6823956442831216,0.6854420537090499,0.6818167271731456,0.6777544226028238,0.6941651817568522,0.6823956442831216,0.6823786265581074,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_18:50:17.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,False,val_loss,min,5,0.6882352941176471,0.6940585499520509,0.6935161392995227,0.6869683379998095,0.6892998756327532,0.6882352941176471,0.6813370069665534,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256_2024-04-07_06:41:25.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,30,0.6830601092896175,0.6965303015234307,0.6787286550962405,0.678364221887296,0.6985229925937796,0.6830601092896175,0.6808742854301562,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_09:09:59.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_07:22:15.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,35,0.6867030965391621,0.696790827496068,0.6837525705971201,0.6766175873947626,0.7012753168714806,0.6867030965391621,0.6804963193294874,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-09_06:21:15.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-09_05:59:38.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,False,val_loss,min,5,0.6764705882352942,0.6915780121204892,0.6738099510942582,0.6743628950218138,0.7004102021634664,0.6764705882352942,0.6802136371422673,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256_2024-04-07_04:06:15.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,35,0.6778398510242085,0.7007322162418652,0.6875512949618487,0.6778253716617157,0.710112974902199,0.6778398510242085,0.6790193481338427,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_12:31:19.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,35,0.6842105263157895,0.6732233450978649,0.6729412545870421,0.6648494859444943,0.688177093923101,0.6842105263157895,0.6790057664023093,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_20:45:15.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,25,0.6757741347905283,0.7208602413863094,0.6692529580936386,0.6748384104162228,0.7215144007894074,0.6757741347905283,0.6787893787783138,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_18:17:17.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_14:11:50.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,20,0.6797020484171322,0.6832737823141274,0.6714429770754673,0.669428235003546,0.6909164289731677,0.6797020484171322,0.6783468414430548,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_21:14:55.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,32,32,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,25,0.6799276672694394,0.6967120527468668,0.6601530063437174,0.6587554631130104,0.7109774114995961,0.6799276672694394,0.6776871369293588,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_05:57:48.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,20,0.6797020484171322,0.6875362232320186,0.6786717784813199,0.6740667832164874,0.6938741856696775,0.6797020484171322,0.6764829063224276,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_12:17:54.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,25,0.6739526411657559,0.7018298955732896,0.6661576522039566,0.670179182929303,0.7059711818954033,0.6739526411657559,0.6763693463733578,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_09:02:32.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_07:22:15.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,30,0.6721311475409836,0.6951127531264684,0.6715449209686224,0.6730064096000773,0.6972735411705355,0.6721311475409836,0.6747905461141375,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-09_06:13:24.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-09_05:59:38.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,20,0.6775956284153005,0.6759418484446533,0.6781718382825529,0.6695257743584372,0.6857354614855665,0.6775956284153005,0.6745625379227874,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-09_07:05:30.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-09_05:59:38.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,20,0.6721311475409836,0.6896386434056476,0.6707238360291994,0.6709041739499384,0.6954575349252443,0.6721311475409836,0.6743451110333627,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_08:56:00.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_07:22:15.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,20,0.6745098039215687,0.6917313034558589,0.6795581814441715,0.674117550532509,0.6970713392598852,0.6745098039215687,0.673538355284982,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_05:08:07.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,20,0.6722532588454376,0.6935493765493764,0.6706320315412309,0.6730458357160186,0.6938363946557615,0.6722532588454376,0.6730685038700941,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_23:08:44.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,25,0.6757741347905283,0.6875282593465037,0.6728029504760141,0.6684271756716975,0.6933246708855371,0.6757741347905283,0.6728016686812566,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_14:48:37.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_14:11:50.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,30,0.6834264432029795,0.6842714892435091,0.6892691449223458,0.67109109306344,0.6911429274548423,0.6834264432029795,0.6727621738811298,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_22:45:21.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,30,0.6751361161524501,0.6732012364622662,0.6650633027295566,0.6610427806141624,0.6844023013891032,0.6751361161524501,0.6725616079339966,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_21:05:04.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,20,0.6739526411657559,0.6921421135381308,0.6709440917109522,0.6675765272233644,0.6952955905646776,0.6739526411657559,0.6705864323788286,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_15:47:42.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_14:11:50.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,30,0.6775956284153005,0.6913817465763707,0.675473803586616,0.6643767474939702,0.7007130459778038,0.6775956284153005,0.6703882439695621,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_16:02:43.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_14:11:50.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,35,0.6705882352941176,0.6780516139848569,0.6686951849847961,0.6621688006544665,0.6891047471454576,0.6705882352941176,0.670069060679413,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_05:25:38.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,25,0.6705882352941176,0.6866668052255516,0.670524937478834,0.6723875875298249,0.6806353328975435,0.6705882352941176,0.6689130452834083,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_05:13:56.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,35,0.6794171220400729,0.6875109204765767,0.6774491659947268,0.6635850827240384,0.6967073077540233,0.6794171220400729,0.6683468805203308,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_20:22:19.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_18:29:03.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,25,0.6660617059891107,0.6691993445454515,0.6458643545962767,0.6492230870399267,0.6823810897457452,0.6660617059891107,0.667059328949517,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_21:02:23.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,20,0.6666666666666666,0.6739848895542544,0.6602941310766429,0.661928750398798,0.6765786514938824,0.6666666666666666,0.6667422031815772,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_04:19:58.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,20,0.6696914700544465,0.6714882263121432,0.6694229934138611,0.6596788183707428,0.6839459233174026,0.6696914700544465,0.6662729012475335,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_18:27:20.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,20,0.6612021857923497,0.679677259150407,0.6593351480781625,0.660302153965842,0.6880015716988137,0.6612021857923497,0.6652903204868483,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_16:57:16.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_14:11:50.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,30,0.6666666666666666,0.6896306899816104,0.6685669896305133,0.6622092876452703,0.6971273405181017,0.6666666666666666,0.6650281955473968,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_22:18:09.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,25,0.668488160291439,0.6810422335449509,0.6653990475771168,0.6612045644504458,0.6833771127221087,0.668488160291439,0.6641790494013738,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_17:06:34.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_14:11:50.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,25,0.663023679417122,0.6754157918500493,0.6614693373504793,0.6614389519259476,0.6790201850901638,0.663023679417122,0.6639847294085449,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_20:07:28.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_18:29:03.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,30,0.6721311475409836,0.6819759261175502,0.6665984453166285,0.6590295720513122,0.6864355549302371,0.6721311475409836,0.6635411822115431,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_19:14:09.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_18:29:03.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,20,0.6685288640595903,0.6791107347060508,0.6685486054941763,0.6612568829118639,0.6822189367494536,0.6685288640595903,0.6634704350142638,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_22:13:08.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,25,0.6666666666666666,0.6914178292805924,0.6612056421584785,0.6598051773719814,0.6925899819044065,0.6666666666666666,0.6628369916433472,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_21:50:14.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-07_20:29:04.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,20,0.6607843137254902,0.6789234134932317,0.6570832011477574,0.6561084854575887,0.6848947631854971,0.6607843137254902,0.6605544337004572,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_02:36:06.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,False,val_loss,min,5,0.6549019607843137,0.6755171794452977,0.6534124466134108,0.6533903109298931,0.6773268842196063,0.6549019607843137,0.6550307944410012,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64_2024-04-07_05:33:07.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,False,val_loss,min,5,0.6612021857923497,0.6554654808507655,0.6575773585388798,0.6483663030530191,0.6640354457074068,0.6612021857923497,0.6549894518930641,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128_2024-04-08_15:14:31.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_14:11:50.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,25,0.6529411764705882,0.663528646890341,0.6526839315291232,0.6486553146223794,0.670215469159115,0.6529411764705882,0.6523899540754066,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_06:30:37.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,10,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,30,0.6573556797020484,0.6580751485701467,0.6575446870533508,0.649808737332187,0.6621188416304273,0.6573556797020484,0.6522192265281441,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_12:26:44.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,30,0.6497277676950998,0.6643039670115456,0.6450324762908421,0.6434305939065255,0.6722531695835119,0.6497277676950998,0.6502246611773155,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_20:42:31.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,25,0.6533575317604355,0.6602490640085434,0.6545945823260346,0.6457596075870875,0.6727611651604085,0.6533575317604355,0.6501147850636707,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_13:53:55.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-05_22:16:40.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,30,0.6612021857923497,0.6629721740764782,0.6565816478165283,0.6484373660672411,0.6597413255116529,0.6612021857923497,0.6493611474050918,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_20:14:34.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_18:29:03.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,30,0.6442831215970962,0.6520105423833715,0.6302763760015234,0.630539954080626,0.6733460504836151,0.6442831215970962,0.6487198607876311,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_18:07:26.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,25,0.663023679417122,0.6931043108288713,0.669233989345224,0.6479372260362826,0.6996090931136785,0.663023679417122,0.6483669673950496,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_19:07:41.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_18:29:03.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,35,0.6575591985428051,0.6593797965491446,0.6545833030585322,0.64339995685612,0.6628457423808917,0.6575591985428051,0.6478101732137894,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-09_07:29:38.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-09_05:59:38.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,35,0.644808743169399,0.673596472411834,0.6403982115052537,0.6412112553222786,0.6790040436339512,0.644808743169399,0.6456998707802638,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_17:26:57.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_14:11:50.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,25,0.6470588235294118,0.6602644870521819,0.6500273268322294,0.6458639641950459,0.662001097532547,0.6470588235294118,0.6456683947146111,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_04:23:54.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,35,0.6424682395644283,0.6501607185504097,0.6341830127805534,0.6309710939372278,0.6671416135018595,0.6424682395644283,0.6442297341450915,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_21:08:01.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,25,0.6460980036297641,0.6446614849980279,0.6307264600637115,0.6307824799314554,0.6527803897580081,0.6460980036297641,0.6427363787854725,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_18:29:40.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,25,0.647912885662432,0.6392230676640696,0.6213641950179887,0.6205254793643078,0.653221047785116,0.647912885662432,0.6415360167732804,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_20:14:46.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,20,0.6372549019607843,0.6451537148735039,0.634831840181082,0.6336659679960498,0.6555338937511348,0.6372549019607843,0.6397324595763965,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_03:21:06.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,30,0.6460980036297641,0.6440991037829272,0.6340802506033177,0.6266538762752794,0.6570152181212634,0.6460980036297641,0.6392847428338109,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_18:56:27.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,35,0.6333333333333333,0.6529413244263067,0.6395012737621808,0.6372628181712973,0.654749346212583,0.6333333333333333,0.635237561696241,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_06:06:28.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,20,0.6352087114337568,0.6336042855340757,0.6326443846664423,0.6254220213368973,0.6465836243910587,0.6352087114337568,0.6323861421396225,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_20:11:21.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,30,0.6411657559198543,0.642710120272715,0.6366041241862777,0.6245316241184592,0.6518635289224511,0.6411657559198543,0.6314577602064411,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-09_07:21:04.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-09_05:59:38.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,25,0.6357012750455373,0.6380580750526238,0.6357347916979694,0.62782364289526,0.6428010304426027,0.6357012750455373,0.6304897522575876,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-09_06:06:40.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-09_05:59:38.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,20,0.6333333333333333,0.6362499933535739,0.6287409866081843,0.6259021008549717,0.6355404457239872,0.6333333333333333,0.6281879240703423,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_06:55:29.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,25,0.6357012750455373,0.6500846939039223,0.6369902437864879,0.6209132753300188,0.6582320082374405,0.6357012750455373,0.6245819375278964,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_15:55:02.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_14:11:50.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,30,0.6176470588235294,0.658291846828031,0.6184886442059607,0.625630267233725,0.6573725678169692,0.6176470588235294,0.6237326039453562,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_02:47:07.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,20,0.6229508196721312,0.6494982926416437,0.6222190625408331,0.620197591712474,0.6559899156325645,0.6229508196721312,0.6236229798395301,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_14:42:54.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_14:11:50.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,20,0.6265938069216758,0.6308263738526472,0.6261970726164637,0.6192227707481973,0.6341580856656637,0.6265938069216758,0.6210624097360128,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_21:03:06.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_18:29:03.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,30,0.6254901960784314,0.6373443544189692,0.6257010086054763,0.6185378807283822,0.6444486455197678,0.6254901960784314,0.6205981603922843,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_04:28:22.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,32,True,val_loss,min,30,0.6215686274509804,0.6562639427226321,0.6220001971724187,0.6192689042439341,0.6582708085735851,0.6215686274509804,0.6202200147404902,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_05:19:32.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,30,0.6247723132969034,0.6541206173508486,0.6192135293327711,0.6129914768309006,0.6606835300289176,0.6247723132969034,0.6190864962741696,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_17:17:40.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_14:11:50.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,30,0.6254901960784314,0.6178940335114879,0.6214811428196125,0.611421924491168,0.626820290346157,0.6254901960784314,0.6178165440845159,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_06:34:16.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,25,0.6176470588235294,0.6352409645312872,0.6250505003641537,0.6195214359945473,0.6384099209058096,0.6176470588235294,0.6177073474986646,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_03:55:02.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,20,0.615686274509804,0.6530448043586936,0.6185148441182446,0.6203002388828354,0.6504885376236631,0.615686274509804,0.6166742949200921,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_06:27:33.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,20,0.6206896551724138,0.603857093007488,0.5964396077503473,0.5936752445223707,0.6208770288158157,0.6206896551724138,0.6153271697003233,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_20:37:22.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,25,0.6211293260473588,0.610154992997567,0.620438793783588,0.607835689401026,0.6204851616890233,0.6211293260473588,0.6133705767821611,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-09_07:12:18.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-09_05:59:38.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,20,0.6193078324225865,0.6298020259914515,0.6146172662545746,0.6086377807241592,0.634146335187228,0.6193078324225865,0.6130229729369748,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-09_05:59:38.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-09_05:59:38.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,20,0.6098003629764065,0.616088382388923,0.5901500751315252,0.5900803856353021,0.6235234280489949,0.6098003629764065,0.6045965294180884,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_20:59:54.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,30,0.604735883424408,0.6180948452777979,0.599549057823995,0.5958478759581107,0.6263629825327521,0.604735883424408,0.6020791673421404,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-08_21:17:46.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_18:29:03.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,25,0.6058823529411764,0.6230184109057537,0.610156700106143,0.6001623612679475,0.6275532507397219,0.6058823529411764,0.6006544109957688,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_05:57:20.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,25,0.6061705989110708,0.6299893665152183,0.5935990860823119,0.5812285376879911,0.6420668712103743,0.6061705989110708,0.5961575712926237,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_18:04:18.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,25,0.5941176470588235,0.6638388835018159,0.6062890617430206,0.5939482591927406,0.6738730044567159,0.5941176470588235,0.5934683871848783,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_03:25:17.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,35,0.5882352941176471,0.6060654406632052,0.5865749506644641,0.5839439132945731,0.6162380394826292,0.5882352941176471,0.5901240337708821,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_02:53:39.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,35,0.5882352941176471,0.6078336296840707,0.5958537894707311,0.5919402706736756,0.610030972228522,0.5882352941176471,0.588755141216737,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_07:05:20.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,35,0.5941176470588235,0.6206733377976306,0.5977243973658826,0.5929957934114769,0.6163765708919597,0.5941176470588235,0.5885914105157428,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_03:33:21.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,30,0.592156862745098,0.6158389741733294,0.5995861860210422,0.5855175996594529,0.6249743018904049,0.592156862745098,0.5842962909087709,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_06:01:26.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,30,0.5862745098039216,0.60540933901821,0.5919756481278997,0.5826038813390647,0.6109168478989796,0.5862745098039216,0.5812985462070243,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_03:58:36.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,True,val_loss,min,35,0.5823529411764706,0.6053339672525043,0.5889398065383145,0.5813667345465876,0.6116716587657673,0.5823529411764706,0.5807502337571148,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_06:37:41.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,20,0.5862745098039216,0.5983095990437168,0.5943199313741259,0.5793674220856246,0.606903395068882,0.5862745098039216,0.579781731447973,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_03:52:31.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,64,True,val_loss,min,20,0.5843137254901961,0.5975670622327182,0.5819267829716709,0.5744109526313754,0.6030587624686209,0.5843137254901961,0.5779887583386845,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_05:53:36.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,128,True,val_loss,min,35,0.5725490196078431,0.6253240803914777,0.5704937247367284,0.5785923095465221,0.6225595013445759,0.5725490196078431,0.5775690113343345,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_04:02:01.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,30,0.5686274509803921,0.5829427738754378,0.570699800664358,0.5662110667203613,0.5866874615735084,0.5686274509803921,0.5676100885535704,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_07:01:45.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,30,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,20,0.5771324863883848,0.5911259530162527,0.5533183738456451,0.5429398165770445,0.6094212333482018,0.5771324863883848,0.5672798128430386,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-06_18:01:44.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,256,True,val_loss,min,25,0.5705882352941176,0.5665254400350224,0.5597109822717223,0.5538256134781963,0.571880918764343,0.5705882352941176,0.5622742131288798,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_06:58:23.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,32,True,val_loss,min,25,0.5666666666666667,0.5853912524664592,0.5779988221535243,0.5600378205360188,0.5982804269684638,0.5666666666666667,0.5598617576407587,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_32__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_02:41:25.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,256,True,val_loss,min,35,0.5470588235294118,0.5599905466097966,0.5495581006637209,0.5446500488204158,0.5698905578733477,0.5470588235294118,0.5477955698653045,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_256__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_04:32:09.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,32,32,40,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,False,200,64,True,val_loss,min,30,0.5490196078431373,0.5601854749357333,0.5512445485576043,0.526865978138034,0.5686765431890335,0.5490196078431373,0.530045904147726,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_64__early_stopping_monitor_val_loss_mode_min_patience_15_2024-04-07_03:29:11.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-06_17:43:13.log
20,64,64,20,42,HARM - ConvLSTM,softmax,categorical_crossentropy,0.005,True,200,128,False,val_loss,min,5,0.1183970856102003,0.1375043465578388,0.117509621949138,0.0917908634459615,0.1378413823201059,0.1183970856102003,0.0926245359233367,/Human_Activity_Video_Recognition/data/output/models/UCF50/human_activity_recognition_model_20_classes_adam_0_005__epochs_200__batch_size_128_2024-04-08_20:30:18.keras,/Human_Activity_Video_Recognition/data/output/logs/model_evaluation/evaluation_log_2024-04-08_18:29:03.log
